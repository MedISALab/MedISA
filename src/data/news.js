export const news = [
  {
    title: "LearnDiff: MRI image super-resolution using a diffusion model with learnable noise",
    authors: ["Sagnik Goswami", "Akriti Gupta1", "Angshuman Paul"],
    year: 2025,
    description: "MRI images with a superior spatial resolution may facilitate an accurate and faster diagnosis. We present LearnDiff, a diffusion probabilistic model with learnable noise for the super-resolution of MRI images. Unlike the standard diffusion models that rely on a fixed, standard normal distribution, LearnDiff utilizes a learnable Gaussian distribution in the diffusion bottleneck, enabling both forward and reverse processes to adapt dynamically. This flexibility addresses a critical limitation. A standard normal distribution for noise may not be adequate in the context of MRI super-resolution using a residual approach. By allowing the noise distribution to be learnable, our model achieves SOTA performance on publicly available MRI images, showing a 3.8% improvement in PSNR compared to previous SOTA methods, significantly outperforming traditional diffusion models. Across multiple MRI datasets, our approach yields superior image quality and enhanced quantitative metrics, highlighting its effectiveness in capturing finer image details and achieving more accurate super-resolution.",
    published: "Computerized Medical Imaging and Graphics"
  },
  {
    title: "Few-Shot Diagnosis of Chest X-Ray Images using Auxiliary Information Guided Semideterministic Infinite Mixture Prototypes",
    authors: ["Prabhala Sandhya Gayatri", "Devi Prasad Maharathy", "Angshuman Paul"],
    year: 2025,
    description: "This paper proposes a few-shot learning (FSL) approach for diagnosing chest X-ray images, designed to work effectively with limited annotated data. The method utilizes auxiliary semantic information related to various chest abnormalities, enabling the model to learn more efficiently and improve diagnostic accuracy even in data-scarce scenarios. A major challenge addressed by this work is the variation in visual characteristics of the same abnormality across different X-ray images. Factors such as patient anatomy, imaging conditions, and disease progression can cause significant differences in how an abnormality appears. Traditional approaches often assume that data points with the same abnormality form a single cluster based solely on visual features, which is ineffective for multi-label datasets where images can contain multiple abnormalities with diverse patterns. To tackle this issue, the authors introduce a semi-deterministic infinite mixture prototype method that dynamically generates multiple clusters for each abnormality. This allows the model to capture the full range of visual variations associated with a condition. Importantly, the clustering process is guided by semantic information, which helps in creating more discriminative and meaningful representations of chest X-ray images. The proposed method was evaluated on publicly available chest X-ray datasets, demonstrating superior performance in identifying and classifying abnormalities with limited training data. By combining few-shot learning with semantic guidance, this approach offers a scalable and accurate solution for medical image analysis",
    published: "Computers in Biology and Medicine"
  },
  {
    title: "Distribution-guided Generative Replay with Semantic Prompts for Class-Incremental Chest X-ray Diagnosi",
    authors: ["Jayant Mahawar", "Devi Prasad Maharathy", "Angshuman Paul"],
    year: 2025,
    description: "Existing class-incremental learning (CIL) methods perform poorly for the diagnosis of medical images. Some CIL approaches require storing exemplars, which raises privacy and storage concerns. Others rely on unconditioned generative replay, compromising discriminative power. To overcome these issues, we propose a new CIL framework for chest x-ray (CXR) diagnosis that combines prompt tuning with distribution-guided generative replay at the feature level. The prompts are initialized with semantically rich embeddings and refined through training. A variational autoencoder captures the feature distribution in latent space, enabling past knowledge retention without storing raw data. To balance stability and plasticity, parts of the network are frozen after the initial phase while others adapt to new classes. In each session, the model learns new classes using real data and replays the synthetic features of old ones to reduce forgetting. A classification module picks the closest class prompt using cosine similarity. We evaluate our method on public CXR datasets. It outperforms prior CIL methods in accuracy and retention. We achieve up to 9\% improvement in average accuracy compared to SOTA methods",
    published: "The 36th British Machine Vision Conference"
  },
  {
    title: "Knowledge Distillation for an Ensemble of Students from a Pyramid of Teachers with Diverse Perspective",
    authors: ["Shilajit Banerjee", "Angshuman Paul"],
    year: 2025,
    description: "Knowledge distillation (KD) can be used for enhancing the performance of a lightweight student models with the help of knowledge from heavier teacher models. Most KD methods for classification use a one-teacher one-student architecture where only one teacher is responsible for transferring knowledge to a student for all the classes. However, when the number of classes increases, it may become difficult for a single teacher to learn the salient characteristics of all the classes. This may also adversely affect the performance of a student in a KD approach. In this paper, we present a novel KD method where an ensemble of lightweight students is trained by a pyramid of teachers. At the top level of the pyramid, we have one teacher that learns all the class labels under consideration. As we go down the pyramid, the number of teachers increases at each level. However, except for the top level, each teacher learns a smaller subset of classes compared to its upper levels. Hence, different teachers learn different perspectives of the classification problem. Also, as we move down the pyramid, the teachers become more and more specialized. On the contrary, as we move upward, the teachers learn a broader and broader perspective about the classification problem. We design a novel distillation loss to distill the knowledge between the student and the pyramid of teachers. Experimental results on publicly available datasets show the effectiveness of the proposed method.",
    published: "IEEE Transactions on Artificial Intelligence"
  },
];